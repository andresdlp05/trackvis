{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eab591c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356263f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.7\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ef4a7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('df_final1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a79ecdbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3,   4,   5,   7,   8,   9,  30,  32,  33,  36,  39,  41,  43,\n",
       "        47,  48,  50,  51,  56,  57,  58,  63,  66,  70,  74,  76,  80,\n",
       "        81,  84,  85,  86,  88,  92,  93, 101, 103, 104, 107, 110, 114,\n",
       "       122, 124, 128, 129, 134, 137, 138, 142, 143, 144, 146,   0,   2,\n",
       "        10,  11,  15,  20,  21,  22,  23,  24,  26,  29,  34,  35,  38,\n",
       "        40,  52,  61,  67,  69,  72,  77,  87,  89,  90,  91,  97,  99,\n",
       "       100, 102, 108, 111, 117, 118, 127, 130, 132, 145, 147,  12,  13,\n",
       "        14,  17,  28,  31,  45,  54,  60,  73,  75,  94,  95,  96, 105,\n",
       "       109, 113, 116, 120, 133, 135, 136, 141, 148,  19,  25,  37,  44,\n",
       "        49,  53,  55,  62,  64,  79,  83, 115, 119, 126, 131, 140,   1,\n",
       "        16,  27,  42,  71,  78,  82,  98, 123, 125, 149,   6,  46,  68,\n",
       "       106, 121, 112, 139,  65,  18,  59])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"ImageName\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc1dc6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ALL_PROJECT_con_clase.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8020ef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3,   4,   5,   7,   8,   9,  30,  32,  33,  36,  39,  41,  43,\n",
       "        47,  48,  50,  51,  56,  57,  58,  63,  66,  70,  74,  76,  80,\n",
       "        81,  84,  85,  86,  88,  92,  93, 101, 103, 104, 107, 110, 114,\n",
       "       122, 124, 128, 129, 134, 137, 138, 142, 143, 144, 146,   0,   2,\n",
       "        10,  11,  15,  20,  21,  22,  23,  24,  26,  29,  34,  35,  38,\n",
       "        40,  52,  61,  67,  69,  72,  77,  87,  89,  90,  91,  97,  99,\n",
       "       100, 102, 108, 111, 117, 118, 127, 130, 132, 145, 147,  12,  13,\n",
       "        14,  17,  28,  31,  45,  54,  60,  73,  75,  94,  95,  96, 105,\n",
       "       109, 113, 116, 120, 133, 135, 136, 141, 148,  19,  25,  37,  44,\n",
       "        49,  53,  55,  62,  64,  79,  83, 115, 119, 126, 131, 140,   1,\n",
       "        16,  27,  42,  71,  78,  82,  98, 123, 125, 149,   6,  46,  68,\n",
       "       106, 121, 112, 139,  65,  18,  59])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"ImageName\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a82378",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7f5679f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vdela\\anaconda3\\python.exe\n",
      "C:\\Users\\vdela\\AppData\\Local\\Programs\\Python\\Python313\\python.exe\n",
      "C:\\Users\\vdela\\AppData\\Local\\Microsoft\\WindowsApps\\python.exe\n"
     ]
    }
   ],
   "source": [
    "!where python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3912aad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/df_todo_con_clase_ratio.csv')\n",
    "ade = pd.read_csv('../data/ade20k_categories.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2943651b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vdela\\anaconda3\\python.exe\n",
      "1.26.4\n"
     ]
    }
   ],
   "source": [
    "import sys, numpy\n",
    "print(sys.executable)\n",
    "print(numpy.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1beeaf8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'python' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m python\u001b[38;5;241m.\u001b[39m__version__\n",
      "\u001b[1;31mNameError\u001b[0m: name 'python' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab028f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf5921f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(869600, 18)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d312e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ade.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7700b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(\n",
    "    ade[['class_id', 'main_class', 'hex_color']]\n",
    "    on='class_id',  \n",
    "    how='left'      \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77355746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.read_csv('../data/df_final.csv')\n",
    "ade = pd.read_csv('../data/ade20k_categories.csv', sep=';')\n",
    "df_final1 = df1.merge(\n",
    "    ade[['class_id', 'main_class', 'hex_color']],\n",
    "    on='class_id',  \n",
    "    how='left'      \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c2b3bd5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_final1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdf_final1\u001b[49m.to_csv(\u001b[33m'\u001b[39m\u001b[33mdf_final1.csv\u001b[39m\u001b[33m'\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'df_final1' is not defined"
     ]
    }
   ],
   "source": [
    "df_final1.to_csv('df_final1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64b54617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time                0\n",
       "ImageIndex          0\n",
       "ImageName           0\n",
       "X                   0\n",
       "Y                   0\n",
       "Z                   0\n",
       "participante        0\n",
       "localX              0\n",
       "localY              0\n",
       "pixelX              0\n",
       "pixelY              0\n",
       "class_id        11133\n",
       "class_name      11133\n",
       "ratio           11133\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.isnull().sum() #11133"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09552596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time                0\n",
       "ImageIndex          0\n",
       "ImageName           0\n",
       "X                   0\n",
       "Y                   0\n",
       "Z                   0\n",
       "participante        0\n",
       "localX              0\n",
       "localY              0\n",
       "pixelX              0\n",
       "pixelY              0\n",
       "class_id        11133\n",
       "class_name      11133\n",
       "ratio           11133\n",
       "main_class      88139\n",
       "hex_color       88139\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba0e71d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0.1         0\n",
       "Unnamed: 0           0\n",
       "Time                 0\n",
       "ImageIndex           0\n",
       "ImageName            0\n",
       "X                    0\n",
       "Y                    0\n",
       "Z                    0\n",
       "participante         0\n",
       "localX               0\n",
       "localY               0\n",
       "pixelX               0\n",
       "pixelY               0\n",
       "class_id        294175\n",
       "class_name      294175\n",
       "ratio           294175\n",
       "main_class      294175\n",
       "hex_color       294175\n",
       "grupo           510044\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658416b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[(df['participante'] == 27) & (df['ImageIndex'] == 0) & (df['ImageName'] == 0)]['main_class'].unique()\n",
    "df[(df['ImageIndex'] == 0) & (df['ImageName'] == 0)]['main_class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73db9d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['ImageIndex'] == 0) & (df['ImageName'] == 0)]['class_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1e63f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando group class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e5352f",
   "metadata": {},
   "source": [
    "![Texto alternativo](grupo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bcc090",
   "metadata": {},
   "outputs": [],
   "source": [
    "ade[\"group_name\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cc221b",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_grupo = {\n",
    "    2: \"Building\", 15: \"Building\", 59: \"Building\", 60: \"Building\",\n",
    "    13: \"Person\",\n",
    "    12: \"Sidewalk\",\n",
    "    21: \"Car\", 81: \"Car\", 103: \"Car\", 84: \"Car\",\n",
    "    5: \"Vegetacion\", 10: \"Vegetacion\", 73: \"Vegetacion\", 18: \"Vegetacion\", 67: \"Vegetacion\",\n",
    "    139: \"Trash\"\n",
    "}\n",
    "\n",
    "# Crear nueva columna\n",
    "df[\"grupo\"] = df[\"class_id\"].map(map_grupo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bbeeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e612c7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['grupo'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583796e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117ccb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbdd57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['ImageIndex']==0) & (df['ImageName']==3) & (df['participante']==1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e30428",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['ImageIndex']==0) & (df['ImageName']==3) & (df['participante']==1)]['class_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d7e9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('df_todo_con_clase_ratio.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cf591d",
   "metadata": {},
   "outputs": [],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac54140",
   "metadata": {},
   "source": [
    "# Inferencias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1075093c",
   "metadata": {},
   "source": [
    "![Texto alternativo](prediction_147.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd90e37",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd582d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd698d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchvision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb056f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "88a56b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CARGA Y ANÁLISIS DE DATOS ===\n",
      "Shape de los datos: (869600, 19)\n",
      "Columnas disponibles: ['Unnamed: 0.1', 'Unnamed: 0', 'Time', 'ImageIndex', 'ImageName', 'X', 'Y', 'Z', 'participante', 'localX', 'localY', 'pixelX', 'pixelY', 'class_id', 'class_name', 'ratio', 'main_class', 'hex_color', 'grupo']\n",
      "\n",
      "Primeras filas:\n",
      "\n",
      "Estadísticas de eye tracking:\n",
      "Número de imágenes únicas: 150\n",
      "Número de participantes únicos: 30\n",
      "Rango de tiempo: 7.360 - 1005.493\n"
     ]
    }
   ],
   "source": [
    "# 1. CARGA Y ANÁLISIS DE DATOS\n",
    "print(\"=== CARGA Y ANÁLISIS DE DATOS ===\")\n",
    "df = pd.read_csv('df_todo_con_clase_ratio.csv')\n",
    "print(f\"Shape de los datos: {df.shape}\")\n",
    "print(f\"Columnas disponibles: {df.columns.tolist()}\")\n",
    "print(f\"\\nPrimeras filas:\")\n",
    "\n",
    "# Análisis de datos de eye tracking\n",
    "print(f\"\\nEstadísticas de eye tracking:\")\n",
    "print(f\"Número de imágenes únicas: {df['ImageName'].nunique()}\")\n",
    "print(f\"Número de participantes únicos: {df['participante'].nunique()}\")\n",
    "print(f\"Rango de tiempo: {df['Time'].min():.3f} - {df['Time'].max():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9993c0d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b58db1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PREPROCESAMIENTO DE DATOS ===\n",
      "Procesando 150 imágenes...\n",
      "Procesando imagen 1/150\n",
      "Procesando imagen 21/150\n",
      "Procesando imagen 41/150\n",
      "Procesando imagen 61/150\n",
      "Procesando imagen 81/150\n",
      "Procesando imagen 101/150\n",
      "Procesando imagen 121/150\n",
      "Procesando imagen 141/150\n",
      "Datos procesados: 150 mapas de atención visual\n"
     ]
    }
   ],
   "source": [
    "# 2. PREPROCESAMIENTO DE DATOS\n",
    "print(\"\\n=== PREPROCESAMIENTO DE DATOS ===\")\n",
    "\n",
    "def create_fixation_heatmap(df_image, image_shape=(224, 224)):\n",
    "    \"\"\"\n",
    "    Crear mapa de calor de fijaciones para una imagen\n",
    "    \"\"\"\n",
    "    heatmap = np.zeros(image_shape)\n",
    "    \n",
    "    if len(df_image) == 0:\n",
    "        return heatmap\n",
    "    \n",
    "    # # Normalizar coordenadas de píxeles a la forma deseada\n",
    "     x_coords = (df_image['pixelX'] * image_shape[1] / 400).astype(int)  # Asumiendo imagen original 400px ancho\n",
    "     y_coords = (df_image['pixelY'] * image_shape[0] / 300).astype(int)  # Asumiendo imagen original 300px alto\n",
    "    # Normalizar coordenadas de píxeles a la forma deseada\n",
    "    #x_coords = (df_image['pixelX'] * image_shape[1] / 800).astype(int)  # Asumiendo imagen original 400px ancho\n",
    "    #y_coords = (df_image['pixelY'] * image_shape[0] / 600).astype(int)  # Asumiendo imagen original 300px alto\n",
    "\n",
    "    # Asegurar que las coordenadas estén dentro de los límites\n",
    "    x_coords = np.clip(x_coords, 0, image_shape[1] - 1)\n",
    "    y_coords = np.clip(y_coords, 0, image_shape[0] - 1)\n",
    "    \n",
    "    # Crear el mapa de calor\n",
    "    for x, y in zip(x_coords, y_coords):\n",
    "        # Aplicar gaussiano para suavizar las fijaciones\n",
    "        sigma = 10\n",
    "        y_range, x_range = np.mgrid[0:image_shape[0], 0:image_shape[1]]\n",
    "        dist = np.sqrt((x_range - x)**2 + (y_range - y)**2)\n",
    "        heatmap += np.exp(-(dist**2) / (2 * sigma**2))\n",
    "    \n",
    "    # Normalizar a [0, 1]\n",
    "    if heatmap.max() > 0:\n",
    "        heatmap = heatmap / heatmap.max()\n",
    "    \n",
    "    return heatmap\n",
    "\n",
    "# Procesar datos para crear mapas de atención visual\n",
    "visual_attention_data = []\n",
    "unique_images = df['ImageName'].unique()\n",
    "\n",
    "print(f\"Procesando {len(unique_images)} imágenes...\")\n",
    "\n",
    "for i, img_name in enumerate(unique_images):\n",
    "    if i % 20 == 0:\n",
    "        print(f\"Procesando imagen {i+1}/{len(unique_images)}\")\n",
    "    \n",
    "    img_data = df[df['ImageName'] == img_name]\n",
    "    \n",
    "    # Crear mapa de atención visual\n",
    "    attention_map = create_fixation_heatmap(img_data)\n",
    "    \n",
    "    # Obtener información adicional de la imagen\n",
    "    participant_count = img_data['participante'].nunique()\n",
    "    fixation_count = len(img_data)\n",
    "    \n",
    "    # Calcular centroide de atención\n",
    "    if len(img_data) > 0:\n",
    "        centroid_x = img_data['pixelX'].mean()\n",
    "        centroid_y = img_data['pixelY'].mean()\n",
    "    else:\n",
    "        centroid_x = centroid_y = 0\n",
    "    \n",
    "    visual_attention_data.append({\n",
    "        'ImageName': img_name,\n",
    "        'attention_map': attention_map,\n",
    "        'participant_count': participant_count,\n",
    "        'fixation_count': fixation_count,\n",
    "        'centroid_x': centroid_x,\n",
    "        'centroid_y': centroid_y\n",
    "    })\n",
    "\n",
    "print(f\"Datos procesados: {len(visual_attention_data)} mapas de atención visual\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ace81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. VISUALIZACIÓN DE EJEMPLOS\n",
    "print(\"\\n=== VISUALIZACIÓN DE EJEMPLOS ===\")\n",
    "\n",
    "def plot_attention_examples(visual_data, n_examples=6):\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Seleccionar ejemplos variados\n",
    "    indices = np.linspace(0, len(visual_data)-1, n_examples, dtype=int)\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        data = visual_data[idx]\n",
    "        attention_map = data['attention_map']\n",
    "        \n",
    "        # Mostrar mapa de atención\n",
    "        im = axes[i].imshow(attention_map, cmap='hot', alpha=0.8)\n",
    "        axes[i].set_title(f\"Imagen: {data['ImageName']}\\n\"\n",
    "                         f\"Participantes: {data['participant_count']}, \"\n",
    "                         f\"Fijaciones: {data['fixation_count']}\")\n",
    "        axes[i].axis('off')\n",
    "        plt.colorbar(im, ax=axes[i], fraction=0.046, pad=0.04)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Mostrar ejemplos\n",
    "plot_attention_examples(visual_attention_data)\n",
    "\n",
    "# 4. ANÁLISIS ESTADÍSTICO\n",
    "print(\"\\n=== ANÁLISIS ESTADÍSTICO ===\")\n",
    "\n",
    "# Estadísticas de atención visual\n",
    "fixation_counts = [data['fixation_count'] for data in visual_attention_data]\n",
    "participant_counts = [data['participant_count'] for data in visual_attention_data]\n",
    "\n",
    "print(f\"Estadísticas de fijaciones por imagen:\")\n",
    "print(f\"  Media: {np.mean(fixation_counts):.2f}\")\n",
    "print(f\"  Mediana: {np.median(fixation_counts):.2f}\")\n",
    "print(f\"  Std: {np.std(fixation_counts):.2f}\")\n",
    "print(f\"  Min-Max: {np.min(fixation_counts)} - {np.max(fixation_counts)}\")\n",
    "\n",
    "print(f\"\\nEstadísticas de participantes por imagen:\")\n",
    "print(f\"  Media: {np.mean(participant_counts):.2f}\")\n",
    "print(f\"  Mediana: {np.median(participant_counts):.2f}\")\n",
    "print(f\"  Std: {np.std(participant_counts):.2f}\")\n",
    "print(f\"  Min-Max: {np.min(participant_counts)} - {np.max(participant_counts)}\")\n",
    "\n",
    "# 5. DATASET PERSONALIZADO PARA PYTORCH\n",
    "class VisualAttentionDataset(Dataset):\n",
    "    def __init__(self, visual_data, transform=None):\n",
    "        self.visual_data = visual_data\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.visual_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.visual_data[idx]\n",
    "        \n",
    "        # Para este ejemplo, usamos el mapa de atención como entrada y salida\n",
    "        # En un caso real, tendrías las imágenes RGB originales como entrada\n",
    "        attention_map = data['attention_map']\n",
    "        \n",
    "        # Simular características ambientales (RGB, depth, semantic)\n",
    "        # En tu implementación real, extraerías estas características de las imágenes reales\n",
    "        rgb_features = np.random.rand(3, 224, 224)  # Placeholder\n",
    "        depth_features = np.random.rand(1, 224, 224)  # Placeholder\n",
    "        semantic_features = np.random.rand(1, 224, 224)  # Placeholder\n",
    "        \n",
    "        # Combinar características\n",
    "        input_features = np.concatenate([rgb_features, depth_features, semantic_features], axis=0)\n",
    "        \n",
    "        if self.transform:\n",
    "            input_features = self.transform(input_features)\n",
    "            attention_map = self.transform(attention_map)\n",
    "        \n",
    "        return {\n",
    "            'input_features': torch.FloatTensor(input_features),\n",
    "            'attention_map': torch.FloatTensor(attention_map).unsqueeze(0),\n",
    "            'image_name': data['ImageName'],\n",
    "            'fixation_count': data['fixation_count']\n",
    "        }\n",
    "\n",
    "# 6. MODELO UNET-CBAM (SIMPLIFICADO)\n",
    "class SimplifiedUNetCBAM(nn.Module):\n",
    "    def __init__(self, in_channels=5, out_channels=1):\n",
    "        super(SimplifiedUNetCBAM, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.encoder2 = nn.Sequential(\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.encoder3 = nn.Sequential(\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder2 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(256, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.decoder1 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(128, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Output\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Conv2d(64, out_channels, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        e1 = self.encoder1(x)\n",
    "        e2 = self.encoder2(e1)\n",
    "        e3 = self.encoder3(e2)\n",
    "        \n",
    "        # Decoder\n",
    "        d2 = self.decoder2(e3)\n",
    "        d1 = self.decoder1(d2)\n",
    "        \n",
    "        # Output\n",
    "        out = self.output(d1)\n",
    "        return out\n",
    "\n",
    "# 7. ENTRENAMIENTO DEL MODELO\n",
    "def train_model():\n",
    "    print(\"\\n=== ENTRENAMIENTO DEL MODELO ===\")\n",
    "    \n",
    "    # Crear dataset\n",
    "    dataset = VisualAttentionDataset(visual_attention_data)\n",
    "    \n",
    "    # Dividir en train/test\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "    \n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "    \n",
    "    # Modelo\n",
    "    model = SimplifiedUNetCBAM()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Entrenamiento\n",
    "    num_epochs = 10\n",
    "    train_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            inputs = batch['input_features']\n",
    "            targets = batch['attention_map']\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    # Evaluación\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    predictions = []\n",
    "    ground_truths = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            inputs = batch['input_features']\n",
    "            targets = batch['attention_map']\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            predictions.extend(outputs.cpu().numpy())\n",
    "            ground_truths.extend(targets.cpu().numpy())\n",
    "    \n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    print(f'Test Loss: {avg_test_loss:.4f}')\n",
    "    \n",
    "    return model, train_losses, predictions, ground_truths\n",
    "\n",
    "# 8. MÉTRICAS DE EVALUACIÓN\n",
    "def calculate_metrics(predictions, ground_truths):\n",
    "    print(\"\\n=== MÉTRICAS DE EVALUACIÓN ===\")\n",
    "    \n",
    "    # Convertir a arrays numpy\n",
    "    pred_flat = np.array(predictions).flatten()\n",
    "    gt_flat = np.array(ground_truths).flatten()\n",
    "    \n",
    "    # MSE y RMSE\n",
    "    mse = mean_squared_error(gt_flat, pred_flat)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    # MAE\n",
    "    mae = mean_absolute_error(gt_flat, pred_flat)\n",
    "    \n",
    "    # Correlación de Pearson\n",
    "    correlation, p_value = pearsonr(pred_flat, gt_flat)\n",
    "    \n",
    "    # SSIM (implementación simplificada)\n",
    "    def ssim_simple(x, y):\n",
    "        mean_x, mean_y = np.mean(x), np.mean(y)\n",
    "        var_x, var_y = np.var(x), np.var(y)\n",
    "        cov_xy = np.mean((x - mean_x) * (y - mean_y))\n",
    "        \n",
    "        c1, c2 = 0.01**2, 0.03**2\n",
    "        ssim = ((2 * mean_x * mean_y + c1) * (2 * cov_xy + c2)) / \\\n",
    "               ((mean_x**2 + mean_y**2 + c1) * (var_x + var_y + c2))\n",
    "        return ssim\n",
    "    \n",
    "    ssim_score = ssim_simple(pred_flat, gt_flat)\n",
    "    \n",
    "    print(f\"MSE: {mse:.6f}\")\n",
    "    print(f\"RMSE: {rmse:.6f}\")\n",
    "    print(f\"MAE: {mae:.6f}\")\n",
    "    print(f\"Correlación de Pearson: {correlation:.6f} (p-value: {p_value:.6f})\")\n",
    "    print(f\"SSIM: {ssim_score:.6f}\")\n",
    "    \n",
    "    return {\n",
    "        'mse': mse,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'correlation': correlation,\n",
    "        'ssim': ssim_score\n",
    "    }\n",
    "\n",
    "# 9. VISUALIZACIÓN DE RESULTADOS\n",
    "def visualize_predictions(predictions, ground_truths, visual_data, n_examples=6):\n",
    "    print(\"\\n=== VISUALIZACIÓN DE PREDICCIONES ===\")\n",
    "    \n",
    "    fig, axes = plt.subplots(3, n_examples, figsize=(20, 12))\n",
    "    \n",
    "    indices = np.random.choice(len(predictions), n_examples, replace=False)\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        pred = predictions[idx][0]  # Remove channel dimension\n",
    "        gt = ground_truths[idx][0]\n",
    "        \n",
    "        # Ground truth\n",
    "        axes[0, i].imshow(gt, cmap='hot')\n",
    "        axes[0, i].set_title(f'Ground Truth {idx}')\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # Prediction\n",
    "        axes[1, i].imshow(pred, cmap='hot')\n",
    "        axes[1, i].set_title(f'Prediction {idx}')\n",
    "        axes[1, i].axis('off')\n",
    "        \n",
    "        # Difference\n",
    "        diff = np.abs(gt - pred)\n",
    "        axes[2, i].imshow(diff, cmap='viridis')\n",
    "        axes[2, i].set_title(f'Difference {idx}')\n",
    "        axes[2, i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 10. EJECUTAR EXPERIMENTO COMPLETO\n",
    "def run_experiment():\n",
    "    print(\"=== INICIANDO EXPERIMENTO DE PREDICCIÓN DE ATENCIÓN VISUAL ===\")\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    model, train_losses, predictions, ground_truths = train_model()\n",
    "    \n",
    "    # Calcular métricas\n",
    "    metrics = calculate_metrics(predictions, ground_truths)\n",
    "    \n",
    "    # Visualizar resultados\n",
    "    visualize_predictions(predictions, ground_truths, visual_attention_data)\n",
    "    \n",
    "    # Gráfico de pérdida durante entrenamiento\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses)\n",
    "    plt.title('Training Loss Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return model, metrics\n",
    "\n",
    "# Ejecutar el experimento\n",
    "if __name__ == \"__main__\":\n",
    "    model, final_metrics = run_experiment()\n",
    "    \n",
    "    print(\"\\n=== RESUMEN FINAL ===\")\n",
    "    print(\"Experimento completado exitosamente!\")\n",
    "    print(\"Métricas finales:\")\n",
    "    for metric, value in final_metrics.items():\n",
    "        print(f\"  {metric.upper()}: {value:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c37859d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b31a91c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d77fe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa81316b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e62ac1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ccb104",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985af253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2e14a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UrbanDataset(Dataset):\n",
    "    def __init__(self, img_dir, fix_dir, labels_df, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.fix_dir = fix_dir\n",
    "        self.labels = labels_df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.labels.iloc[idx]\n",
    "        img_path = os.path.join(self.img_dir, row[\"image\"])\n",
    "        fix_path = os.path.join(self.fix_dir, row[\"fixation_map\"])\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        fixation = Image.open(fix_path).convert(\"L\")  # grayscale fixation GT\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            fixation = self.transform(fixation)\n",
    "\n",
    "        # perception labels (wealthy, safe, etc.)\n",
    "        y = torch.tensor(row[[\"wealthy\",\"safe\",\"lively\",\"beautiful\",\"boring\",\"depressing\"]].values, dtype=torch.float32)\n",
    "\n",
    "        return img, fixation, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9945d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec094e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "# Depth (ejemplo placeholder: salida monocanal)\n",
    "# Puedes integrar Monodepth2 oficial aquí.\n",
    "def get_depth_feature(img_tensor):\n",
    "    return torch.mean(img_tensor, dim=1, keepdim=True)  # dummy depth\n",
    "\n",
    "# Semantic con DeepLabV3\n",
    "sem_model = models.segmentation.deeplabv3_resnet101(pretrained=True).eval()\n",
    "\n",
    "def get_semantic_feature(img_tensor):\n",
    "    with torch.no_grad():\n",
    "        out = sem_model(img_tensor)[\"out\"]\n",
    "    sem_map = out.argmax(1).unsqueeze(1).float()\n",
    "    return sem_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83d2f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class UNetVAP(nn.Module):\n",
    "    def __init__(self, in_channels=5, out_channels=1):\n",
    "        super().__init__()\n",
    "        self.enc1 = nn.Sequential(nn.Conv2d(in_channels,64,3,padding=1), nn.ReLU())\n",
    "        self.enc2 = nn.Sequential(nn.Conv2d(64,128,3,padding=1), nn.ReLU())\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.dec1 = nn.ConvTranspose2d(128,64,2,stride=2)\n",
    "        self.outc = nn.Conv2d(64, out_channels,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool(e1))\n",
    "        d1 = self.dec1(e2)\n",
    "        out = self.outc(d1 + e1)  # skip connection\n",
    "        return torch.sigmoid(out)\n",
    "\n",
    "vap_model = UNetVAP(in_channels=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656510dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(vap_model.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e67949",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGPerception(nn.Module):\n",
    "    def __init__(self, num_outputs=6):\n",
    "        super().__init__()\n",
    "        base = models.vgg16(pretrained=True).features\n",
    "        self.features = base\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512*7*7, 4096), nn.ReLU(True),\n",
    "            nn.Linear(4096, 4096), nn.ReLU(True),\n",
    "            nn.Linear(4096, num_outputs)  # regresión percepciones\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.fc(x)\n",
    "\n",
    "perception_model = VGGPerception(num_outputs=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cfbd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optim_p = torch.optim.Adam(perception_model.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99953d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "for imgs, fix_gt, y in dataloader:\n",
    "    # Extraer features\n",
    "    depth = get_depth_feature(imgs)\n",
    "    sem = get_semantic_feature(imgs)\n",
    "\n",
    "    # Input al VAP\n",
    "    vap_in = torch.cat([imgs, depth, sem], dim=1)\n",
    "    pred_fix = vap_model(vap_in)\n",
    "\n",
    "    # Input al perception model\n",
    "    multi_in = torch.cat([imgs, depth, sem, pred_fix], dim=1)\n",
    "    preds = perception_model(multi_in)\n",
    "\n",
    "    # Loss percepción\n",
    "    loss = loss_fn(preds, y)\n",
    "    loss.backward()\n",
    "    optim_p.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3efa0ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4103b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df1 = pd.read_csv('df_final1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48bfedcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>ImageIndex</th>\n",
       "      <th>ImageName</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>Z</th>\n",
       "      <th>participante</th>\n",
       "      <th>localX</th>\n",
       "      <th>localY</th>\n",
       "      <th>pixelX</th>\n",
       "      <th>pixelY</th>\n",
       "      <th>class_id</th>\n",
       "      <th>class_name</th>\n",
       "      <th>ratio</th>\n",
       "      <th>main_class</th>\n",
       "      <th>hex_color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.984</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>1.0046</td>\n",
       "      <td>1.9762</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>-0.0954</td>\n",
       "      <td>405.2</td>\n",
       "      <td>204.6</td>\n",
       "      <td>21.0</td>\n",
       "      <td>class_21</td>\n",
       "      <td>0.020292</td>\n",
       "      <td>car</td>\n",
       "      <td>#0066C8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.017</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>1.0060</td>\n",
       "      <td>1.9764</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>-0.0940</td>\n",
       "      <td>405.2</td>\n",
       "      <td>206.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>class_21</td>\n",
       "      <td>0.020292</td>\n",
       "      <td>car</td>\n",
       "      <td>#0066C8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.052</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>1.0070</td>\n",
       "      <td>1.9766</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>-0.0930</td>\n",
       "      <td>405.3</td>\n",
       "      <td>207.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>class_21</td>\n",
       "      <td>0.020292</td>\n",
       "      <td>car</td>\n",
       "      <td>#0066C8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.084</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>1.0070</td>\n",
       "      <td>1.9768</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>-0.0930</td>\n",
       "      <td>405.4</td>\n",
       "      <td>207.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>class_21</td>\n",
       "      <td>0.020292</td>\n",
       "      <td>car</td>\n",
       "      <td>#0066C8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.117</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0056</td>\n",
       "      <td>1.0059</td>\n",
       "      <td>1.9766</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0056</td>\n",
       "      <td>-0.0941</td>\n",
       "      <td>405.6</td>\n",
       "      <td>205.9</td>\n",
       "      <td>21.0</td>\n",
       "      <td>class_21</td>\n",
       "      <td>0.020292</td>\n",
       "      <td>car</td>\n",
       "      <td>#0066C8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Time  ImageIndex  ImageName       X       Y       Z  participante  localX  \\\n",
       "0  7.984           0          3  0.0052  1.0046  1.9762             1  0.0052   \n",
       "1  8.017           0          3  0.0052  1.0060  1.9764             1  0.0052   \n",
       "2  8.052           0          3  0.0053  1.0070  1.9766             1  0.0053   \n",
       "3  8.084           0          3  0.0054  1.0070  1.9768             1  0.0054   \n",
       "4  8.117           0          3  0.0056  1.0059  1.9766             1  0.0056   \n",
       "\n",
       "   localY  pixelX  pixelY  class_id class_name     ratio main_class hex_color  \n",
       "0 -0.0954   405.2   204.6      21.0   class_21  0.020292        car   #0066C8  \n",
       "1 -0.0940   405.2   206.0      21.0   class_21  0.020292        car   #0066C8  \n",
       "2 -0.0930   405.3   207.0      21.0   class_21  0.020292        car   #0066C8  \n",
       "3 -0.0930   405.4   207.0      21.0   class_21  0.020292        car   #0066C8  \n",
       "4 -0.0941   405.6   205.9      21.0   class_21  0.020292        car   #0066C8  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a1d7e41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "main_class\n",
       "wall         177\n",
       "floor         39\n",
       "plant         25\n",
       "sky           17\n",
       "tree          12\n",
       "flowerpot      3\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[(df1['participante'] == 6)&(df1['ImageName'] == 1)]['main_class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bf081a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "main_class\n",
       "wall        177\n",
       "graffiti    160\n",
       "floor        39\n",
       "plant        25\n",
       "sky          17\n",
       "tree         12\n",
       "pot           3\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[(df1['participante'] == 6)&(df1['ImageName'] == 1)]['main_class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "edf67401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "main_class\n",
       "tree     157\n",
       "wall     150\n",
       "plant     88\n",
       "sky       33\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[(df1['participante'] == 6)&(df1['ImageName'] == 1)]['main_class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d1c9403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time                0\n",
       "ImageIndex          0\n",
       "ImageName           0\n",
       "X                   0\n",
       "Y                   0\n",
       "Z                   0\n",
       "participante        0\n",
       "localX              0\n",
       "localY              0\n",
       "pixelX              0\n",
       "pixelY              0\n",
       "class_id        11133\n",
       "class_name      11133\n",
       "ratio           11133\n",
       "main_class      88139\n",
       "hex_color       88139\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.isnull().sum() #11133"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d27e6887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(869600, 16)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9357a11a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "visEyeTraking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
